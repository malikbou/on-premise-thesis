# Copy this file to .env and fill in values as needed. Do NOT commit your real .env.

# Where LiteLLM should proxy Ollama requests (Mac host or containerized VM)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# LiteLLM service base URL (inside compose network)
LITELLM_API_BASE=http://litellm:4000

# Retrieval embedding model used for index building and RAG API instances
EMBEDDING_MODEL=nomic-embed-text

# Optional: OpenAI / Anthropic / Google keys for LiteLLM cloud routing
#OPENAI_API_KEY=your_openai_key
#ANTHROPIC_API_KEY=your_anthropic_key
#GOOGLE_API_KEY=your_google_key

# Benchmarker mapping of embedding model -> RAG API base URL (use internal service names)
EMBEDDING_API_MAP=all-minilm=http://rag-api-minilm:8000,nomic-embed-text=http://rag-api-nomic:8000

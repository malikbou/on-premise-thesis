{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d76f706",
   "metadata": {},
   "source": [
    "# Testset Generation\n",
    "The purpose of this script is to generate a testset on a set of documents or document which can then be used to benchmark the performance of the SLMs locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fff847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in /Users/Malik_1/code/malikbou/ucl/thesis/ragas-test/.venv/lib/python3.11/site-packages (1.26.3)\n",
      "Requirement already satisfied: pymupdf in /Users/Malik_1/code/malikbou/ucl/thesis/ragas-test/.venv/lib/python3.11/site-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b66c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, DirectoryLoader, TextLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "# import libraries required\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import fitz\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper, BaseRagasLLM\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper, BaseRagasEmbeddings\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.graph import KnowledgeGraph, Node\n",
    "from ragas.testset.synthesizers import QueryDistribution\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop import (\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    ")\n",
    "from ragas.testset.persona import Persona\n",
    "\n",
    "\n",
    "# Import graph construction components\n",
    "from ragas.testset.transforms import (\n",
    "    apply_transforms,\n",
    "    Parallel,\n",
    "    KeyphrasesExtractor,\n",
    "    SummaryExtractor,\n",
    "    EmbeddingExtractor,\n",
    ")\n",
    "from ragas.testset.transforms.extractors import NERExtractor\n",
    "from ragas.testset.transforms.relationship_builders.cosine import (\n",
    "    SummaryCosineSimilarityBuilder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23017bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "def load_documents(path: str) -> List:\n",
    "    \"\"\"Load documents from a path (file or directory).\"\"\"\n",
    "    if os.path.isdir(path):\n",
    "        # Load all PDFs / MD / txt recursively\n",
    "        loader = DirectoryLoader(path, glob=\"**/*.*\")\n",
    "    elif path.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path)\n",
    "    else:\n",
    "        loader = TextLoader(path)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "file_path = \"data/sts-student-handbook.pdf\"\n",
    "print(f\"Loading documents from {file_path} …\")\n",
    "raw_docs = load_documents(file_path)\n",
    "raw_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c4599",
   "metadata": {},
   "source": [
    "### Semantic Chunking Strategy for the docs instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "\n",
    "def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Chunks documents based on semantic headings rather than fixed size.\n",
    "    It assumes headings are numbered (e.g., \"1. Introduction\", \"2.1 Key Policies\")\n",
    "    and uses these as boundaries for creating meaningful chunks.\n",
    "\n",
    "    If no numbered headings are found, it falls back to the original fixed-size\n",
    "    chunking method.\n",
    "    \"\"\"\n",
    "    print(\"Attempting to chunk documents by semantic headings...\")\n",
    "\n",
    "    # 1. Combine all page content into a single string to handle sections\n",
    "    # that span across pages.\n",
    "    full_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # 2. Define a regex pattern to find numbered headings (e.g., 1. , 1.1 , 1.1.1)\n",
    "    # This pattern looks for lines starting with digits, followed by a dot, a space,\n",
    "    # and then the title.\n",
    "    heading_pattern = re.compile(r\"^\\d+(\\.\\d+)*\\s.*$\", re.MULTILINE)\n",
    "\n",
    "    # Find the start index of all headings\n",
    "    heading_indices = [match.start() for match in heading_pattern.finditer(full_text)]\n",
    "\n",
    "    # 3. If no headings are found, fall back to the original method.\n",
    "    if not heading_indices:\n",
    "        print(\"WARN: No numbered headings found. Falling back to fixed-size chunking.\")\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        return splitter.split_documents(docs)\n",
    "\n",
    "    # 4. Create chunks based on the text between headings.\n",
    "    semantic_chunks = []\n",
    "    for i in range(len(heading_indices)):\n",
    "        # The start of the chunk is the start of the current heading\n",
    "        start_index = heading_indices[i]\n",
    "        # The end of the chunk is the start of the next heading, or the end of the document\n",
    "        end_index = heading_indices[i+1] if i + 1 < len(heading_indices) else len(full_text)\n",
    "\n",
    "        chunk_text = full_text[start_index:end_index].strip()\n",
    "\n",
    "        if chunk_text:\n",
    "            # Create a new Document for each semantic chunk.\n",
    "            # We lose the specific page number but gain immense semantic value.\n",
    "            # The source metadata is preserved from the first document.\n",
    "            semantic_chunks.append(Document(\n",
    "                page_content=chunk_text,\n",
    "                metadata={\"source\": docs[0].metadata.get(\"source\", \"Unknown\")}\n",
    "            ))\n",
    "\n",
    "    print(f\"Successfully created {len(semantic_chunks)} semantic chunks.\")\n",
    "    return semantic_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = chunk_documents(raw_docs)\n",
    "print(f\"Chunked to {len(docs)} passages (≈{CHUNK_SIZE} chars each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a239139",
   "metadata": {},
   "source": [
    "### New Chunking Strategy\n",
    "We are going to try something different here, where the goal will be to convert the PDF into clear markdown format\n",
    "in order to get much better retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f917337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Better PDF Text Extraction\n",
    "pdf_path = 'data/sts-student-handbook.pdf'\n",
    "full_text = \"\"\n",
    "\n",
    "# Open the PDF file\n",
    "with fitz.open(pdf_path) as doc:\n",
    "    # Iterate through each page and extract its text\n",
    "    for page in doc:\n",
    "        full_text += page.get_text() + \"\\n\\n\" # Add page breaks for clarity\n",
    "\n",
    "print(\"Successfully extracted text from PDF.\")\n",
    "print(f\"First 500 characters:\\n---\\n{full_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ef0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "# generator_model = \"gpt-3.5-turbo-16k\"\n",
    "# generator_model = \"gpt-4.1-mini\"\n",
    "generator_model = \"gpt-4o\"\n",
    "\n",
    "# Wrap OpenAI models for Ragas\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=generator_model, temperature=0))\n",
    "embedding_model = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# --- Define Knowledge Graph Transformations ---\n",
    "print(\"Defining Knowledge Graph transformations…\")\n",
    "transforms = [\n",
    "    Parallel(\n",
    "        KeyphrasesExtractor(llm=generator_llm),\n",
    "        SummaryExtractor(llm=generator_llm),\n",
    "        NERExtractor(llm=generator_llm),\n",
    "    ),\n",
    "    EmbeddingExtractor(\n",
    "        embedding_model=embedding_model,\n",
    "        embed_property_name=\"summary\",  # Embed summaries, not full content\n",
    "        property_name=\"summary_embedding\",  # Store embeddings in this property\n",
    "    ),\n",
    "    SummaryCosineSimilarityBuilder(\n",
    "        threshold=0.7  # Lowered threshold to encourage relationship formation\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testset Generation ---\n",
    "\n",
    "# Create UCL student personas\n",
    "persona_new_student = Persona(\n",
    "    name=\"New Student\",\n",
    "    role_description=\"A first-year student looking for information about university policies and procedures.\"\n",
    ")\n",
    "persona_international = Persona(\n",
    "    name=\"International Student\",\n",
    "    role_description=\"An international student seeking information about visa requirements, accommodation, and support services.\"\n",
    ")\n",
    "persona_graduate = Persona(\n",
    "    name=\"Graduate Student\",\n",
    "    role_description=\"A graduate student interested in research opportunities, funding, and academic progression.\"\n",
    ")\n",
    "\n",
    "personas = [persona_new_student, persona_international, persona_graduate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm,\n",
    "    embedding_model=embedding_model,\n",
    "    persona_list=personas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7068206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_query_distribution(\n",
    "    llm: BaseRagasLLM, force_single_hop: bool = False\n",
    ") -> QueryDistribution:\n",
    "    \"\"\"Create a custom query distribution with a mix of question types.\"\"\"\n",
    "    single_hop_specific = SingleHopSpecificQuerySynthesizer(llm=llm)\n",
    "\n",
    "    if not force_single_hop:\n",
    "        multi_hop_specific = MultiHopSpecificQuerySynthesizer(llm=llm)\n",
    "        multi_hop_abstract = MultiHopAbstractQuerySynthesizer(llm=llm)\n",
    "        return [\n",
    "            (single_hop_specific, 0.5),\n",
    "            (multi_hop_specific, 0.25),\n",
    "            (multi_hop_abstract, 0.25),\n",
    "        ]\n",
    "    else:\n",
    "        return [(single_hop_specific, 1.0)]\n",
    "\n",
    "# Define ideal and fallback distributions\n",
    "ideal_distribution = custom_query_distribution(generator_llm)\n",
    "fallback_distribution = custom_query_distribution(generator_llm, force_single_hop=True)\n",
    "\n",
    "print(\"Generating test-set… this may take a few minutes.\")\n",
    "try:\n",
    "    print(\"Attempting to generate test set with multi-hop questions...\")\n",
    "    dataset = generator.generate_with_langchain_docs(\n",
    "        documents=docs,\n",
    "        testset_size=10,\n",
    "        transforms=transforms,\n",
    "        query_distribution=ideal_distribution,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    if \"No clusters found\" in str(e):\n",
    "        print(\"WARN: Could not generate multi-hop questions. Falling back to single-hop only.\")\n",
    "        dataset = generator.generate_with_langchain_docs(\n",
    "            documents=docs,\n",
    "            testset_size=10,\n",
    "            transforms=transforms,\n",
    "            query_distribution=fallback_distribution,\n",
    "        )\n",
    "    else:\n",
    "        # Re-raise any other unexpected errors\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2847b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"testset\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, f\"testset_gpt-4o.json\")\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(dataset.to_list(), f, indent=2)\n",
    "print(f\"Saved {len(dataset.to_list())} samples to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2651ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
